# DeepseekFromScratch
Reimplementing DeepSeek-V3 from scratch â€” attention, latent attention with RoPE, mixture of experts, multi-token prediction, and quantization â€” built step-by-step.

# DeepSeek-V3 from Scratch

ğŸš€ A complete reimplementation of the **DeepSeek-V3** large language model architecture â€” built from the ground up with clear explanations, whiteboard-style derivations, and fully functional code for each module.

This project follows the paper:  
[ğŸ”— DeepSeek-V3 Technical Report (2024)](https://arxiv.org/abs/2405.20183)

---

## ğŸ” What This Project Covers

The DeepSeek-V3 paper is dense and packed with innovation. This repository breaks it down into digestible pieces and reconstructs each idea from first principles, including:

### 1. ğŸ§  Attention Mechanism Foundations
- Self-Attention
- Causal (Masked) Attention
- Multi-Head Attention
- Token flow through Transformer blocks

### 2. ğŸ”‘ Multi-Head Latent Attention with RoPE
- Key-Value Cache and Reuse
- Multi-Query & Grouped Query Attention
- Latent Attention formulation
- RoPE: Rotary Positional Embeddings
- **Decoupled RoPE + Latent Attention**, rederived and coded

### 3. ğŸ§© Mixture of Experts (MoE)
- Classic MoE: Router, Expert selection
- Load Balancing: Auxiliary Loss and Uniform Routing
- **Auxiliary-Loss-Free Load Balancing** (DeepSeek innovation)
- **Shared Experts & Fine-Grained Expert Segmentation**
- MoE block coded from scratch

### 4. ğŸ”® Multi-Token Prediction
- Predicting multiple future tokens instead of one
- Use in pretraining vs inference
- End-to-end implementation

### 5. âš™ï¸ Quantization Techniques
- Mixed Precision Framework
- Fine-Grained Quantization
- Increased Accumulation Precision
- Mantissa/Exponent Tuning
- Online Quantization methods

---

## ğŸ—‚ï¸ Folder Structure

```text
â”œâ”€â”€ 01_attention/
â”œâ”€â”€ 02_latent_attention/
â”œâ”€â”€ 03_moe/
â”œâ”€â”€ 04_multi_token_prediction/
â”œâ”€â”€ 05_quantization/
â”œâ”€â”€ utils/
â””â”€â”€ README.md
```

## ğŸ—‚ï¸ Project Structure

Each directory contains:
- ğŸ§  **Whiteboard-First Notebooks**: Visual explanations, derivations, and breakdowns of complex ideas from the DeepSeek-V3 paper.
- ğŸ’» **Code Implementations**: Clean, from-scratch NumPy (and optionally PyTorch) code to replicate each architectural module.
- ğŸ§ª **Test Scripts**: Minimal test harnesses to validate correctness.
- ğŸ“„ **Notes & Comments**: Inline comments and markdown notes to help with revision and conceptual clarity.

---

## ğŸš€ Run Any Module Independently

Each module is standalone and can be executed independently.

```bash
cd 01_attention
python multi_head_attention.py
```


## ğŸ¯ Why This Project?

This project aims to:

- Provide a detailed, engineering-first walkthrough of DeepSeek-V3's architectural innovations.
- Enable developers and researchers to build efficient LLMs by understanding the internals of key components like latent attention, rotary embeddings, MoE, and quantization.
- Serve as a foundational codebase and learning platform for building your own transformer-based foundational models from scratch.

---

## ğŸ‘¤ Author

**Prateek Pani**  
Machine Learning Researcher 
ğŸ“« [Website](https://prtk1729.github.io/)

---

## ğŸ¤ Contributions Welcome

You are welcome to contribute to:

- Improving or simplifying code
- Adding benchmarks or profiling
- Enhancing documentation and visualizations
- Extending the repository to cover inference and post-training (e.g., RLHF)

To contribute:
1. Fork the repository
2. Create a new branch (`git checkout -b feature-xyz`)
3. Commit your changes and open a pull request

Please open an issue first to discuss major ideas before implementation.

---

## ğŸ“š References

- [DeepSeek-V3: Scaling Open-Source LLMs to 236B Parameters](https://arxiv.org/abs/2405.20183)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Rotary Positional Embeddings (RoPE)](https://blog.eleuther.ai/rotary-embeddings/)
- [GShard and Switch Transformer (Mixture of Experts)](https://arxiv.org/abs/2101.03961)
- [Metaâ€™s Multi-Token Prediction Pretraining (2024)](https://arxiv.org/abs/2401.03460)
